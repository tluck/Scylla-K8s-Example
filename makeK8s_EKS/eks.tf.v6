terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">= 6.0.0" # Adjust if your environment uses 5.x
    }

    random = {
      source  = "hashicorp/random"
      version = ">= 3.6.1"
    }

    tls = {
      source  = "hashicorp/tls"
      version = ">= 4.0.5"
    }

    cloudinit = {
      source  = "hashicorp/cloudinit"
      version = ">= 2.3.4"
    }
  }

  required_version = ">= 1.3"
}

# Create a new VPC
resource "aws_vpc" "new_vpc" {
  cidr_block           = var.vpc_cidr_block
  enable_dns_support   = true
  enable_dns_hostnames = true

  tags = {
    Name  = "${local.cluster_name}-vpc"
    Owner = "sa_demo_scylladb_com"
  }
}

# Public subnets
resource "aws_subnet" "public_subnet_1" {
  vpc_id                  = aws_vpc.new_vpc.id
  cidr_block              = var.subnet_cidr_block1
  availability_zone       = "${var.region}a"
  map_public_ip_on_launch = true

  tags = {
    Name                                      = "${local.cluster_name}-public-subnet-1"
    Owner                                     = "sa_demo_scylladb_com"
    "kubernetes.io/role/elb"                 = "1"
    "kubernetes.io/cluster/${local.cluster_name}" = "shared"
  }
}

resource "aws_subnet" "public_subnet_2" {
  vpc_id                  = aws_vpc.new_vpc.id
  cidr_block              = var.subnet_cidr_block2
  availability_zone       = "${var.region}b"
  map_public_ip_on_launch = true

  tags = {
    Name                                      = "${local.cluster_name}-public-subnet-2"
    Owner                                     = "sa_demo_scylladb_com"
    "kubernetes.io/role/elb"                 = "1"
    "kubernetes.io/cluster/${local.cluster_name}" = "shared"
  }
}

resource "aws_subnet" "public_subnet_3" {
  vpc_id                  = aws_vpc.new_vpc.id
  cidr_block              = var.subnet_cidr_block3
  availability_zone       = "${var.region}c"
  map_public_ip_on_launch = true

  tags = {
    Name                                      = "${local.cluster_name}-public-subnet-3"
    Owner                                     = "sa_demo_scylladb_com"
    "kubernetes.io/role/elb"                 = "1"
    "kubernetes.io/cluster/${local.cluster_name}" = "shared"
  }
}

# Internet gateway and public route table
resource "aws_internet_gateway" "igw" {
  vpc_id = aws_vpc.new_vpc.id

  tags = {
    Name  = "${local.cluster_name}-igw"
    Owner = "sa_demo_scylladb_com"
  }
}

resource "aws_route_table" "public_rt" {
  vpc_id = aws_vpc.new_vpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.igw.id
  }

  tags = {
    Name  = "${local.cluster_name}-public-rt"
    Owner = "sa_demo_scylladb_com"
  }
}

resource "aws_route_table_association" "public_subnet_1_assoc" {
  subnet_id      = aws_subnet.public_subnet_1.id
  route_table_id = aws_route_table.public_rt.id
}

resource "aws_route_table_association" "public_subnet_2_assoc" {
  subnet_id      = aws_subnet.public_subnet_2.id
  route_table_id = aws_route_table.public_rt.id
}

resource "aws_route_table_association" "public_subnet_3_assoc" {
  subnet_id      = aws_subnet.public_subnet_3.id
  route_table_id = aws_route_table.public_rt.id
}

# EKS optimized AMIs
data "aws_ssm_parameter" "eks_ami_x86" {
  name = "/aws/service/eks/optimized-ami/${var.eks_nodegroup_version}/amazon-linux-2023/x86_64/standard/recommended/release_version"
}

data "aws_ssm_parameter" "eks_ami_arm64" {
  name = "/aws/service/eks/optimized-ami/${var.eks_nodegroup_version}/amazon-linux-2023/arm64/standard/recommended/release_version"
}

locals {
  eks_cluster_security_group_id = module.eks.cluster_security_group_id
  eks_node_security_group_id    = module.eks.node_security_group_id

  spark_sg_rules = var.enable_application ? {
    ingress_allow_sparkmaster = {
      type        = "ingress"
      description = "SparkMaster"
      from_port   = 7077
      to_port     = 7077
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }

    ingress_allow_sparkuidriver = {
      type        = "ingress"
      description = "SparkUIDriver"
      from_port   = 4040
      to_port     = 4040
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }

    ingress_allow_sparkuiexecutor = {
      type        = "ingress"
      description = "SparkUIExecutor"
      from_port   = 4041
      to_port     = 4049
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }

    ingress_allow_sparkhistory = {
      type        = "ingress"
      description = "SparkHistory"
      from_port   = 18080
      to_port     = 18080
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }
  } : {}
}

# Existing keypair
resource "aws_key_pair" "key_pair" {
  key_name   = "${local.cluster_name}-keypair"
  public_key = file(var.ssh_public_key_file)
}

# EKS module
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = ">= 21.0.1"

  name                                     = local.cluster_name
  kubernetes_version                       = var.eks_cluster_version
  endpoint_public_access                   = true
  enable_cluster_creator_admin_permissions = true
  create_kms_key                           = false
  create_cloudwatch_log_group              = false
  enabled_log_types                        = []
  enable_irsa                              = true

  vpc_id = aws_vpc.new_vpc.id
  subnet_ids = [
    aws_subnet.public_subnet_1.id,
    aws_subnet.public_subnet_2.id,
    aws_subnet.public_subnet_3.id
  ]

  tags = {
    Owner = "sa_demo_scylladb_com"
    Name  = "sa-demo-cluster"
  }

  addons = {
    kube-proxy = {
      service_account_role_arn = aws_iam_role.irsa_kube_proxy.arn
      most_recent_version      = true
      resolve_conflicts        = "OVERWRITE"
    },
    vpc-cni = {
      # service_account_role_arn = aws_iam_role.vpc_cni_pod_identity.arn
      most_recent_version      = true
      before_compute           = true
      resolve_conflicts_on_create = "OVERWRITE"
      resolve_conflicts        = "OVERWRITE"
    },
    eks-pod-identity-agent = {
      most_recent_version = true
      before_compute      = true
      resolve_conflicts   = "OVERWRITE"
    }
  }

  node_security_group_additional_rules = merge(
    {
      ingress_allow_ssh = {
        type        = "ingress"
        description = "SSH"
        from_port   = 22
        to_port     = 22
        protocol    = "tcp"
        cidr_blocks = ["0.0.0.0/0"]
      }

      ingress_allow_https = {
        type        = "ingress"
        description = "HTTPS"
        from_port   = 443
        to_port     = 443
        protocol    = "tcp"
        cidr_blocks = [var.vpc_cidr_block]
      }

      ingress_allow_webhook = {
        type        = "ingress"
        description = "Webhook"
        from_port   = 5000
        to_port     = 5000
        protocol    = "tcp"
        cidr_blocks = ["0.0.0.0/0"]
      }

      ingress_allow_prometheus = {
        type        = "ingress"
        description = "Prometheus"
        from_port   = 9090
        to_port     = 9090
        protocol    = "tcp"
        cidr_blocks = ["0.0.0.0/0"]
      }

      ingress_allow_grafana = {
        type        = "ingress"
        description = "Grafana"
        from_port   = 3000
        to_port     = 3000
        protocol    = "tcp"
        cidr_blocks = ["0.0.0.0/0"]
      }
    },
    local.spark_sg_rules
  )
}

# Launch template for dedicated node group
resource "aws_launch_template" "group_lt_0" {
  name    = "${var.prefix}group-eks-launch-template-0"
  key_name = aws_key_pair.key_pair.key_name

  block_device_mappings {
    device_name = "/dev/xvda"
    ebs {
      volume_size = var.ebsSize
      volume_type = "gp3"
    }
  }

  user_data = base64encode(<<-EOT
    MIME-Version: 1.0
    Content-Type: multipart/mixed; boundary="BOUNDARY"

    --BOUNDARY
    Content-Type: application/node.eks.aws

    apiVersion: node.eks.aws/v1alpha1
    kind: NodeConfig
    spec:
      cluster:
        name: ${module.eks.cluster_name}
        apiServerEndpoint: ${module.eks.cluster_endpoint}
        certificateAuthority: ${module.eks.cluster_certificate_authority_data}
        cidr: ${module.eks.cluster_service_cidr}
      kubelet:
        config:
          cpuManagerPolicy: static
        systemReserved:
          cpu: "500m"
          memory: "1Gi"
        kubeReserved:
          cpu: "500m"
          memory: "1Gi"
    --BOUNDARY
    Content-Type: text/x-shellscript

    #!/bin/bash
    rm -f /var/lib/kubelet/cpu_manager_state
    --BOUNDARY--
  EOT
  )

  metadata_options {
    http_tokens               = "required"
    http_put_response_hop_limit = 2
    http_endpoint             = "enabled"
  }

  network_interfaces {
    security_groups       = [local.eks_node_security_group_id]
    delete_on_termination = true
  }

  tag_specifications {
    resource_type = "instance"
    tags = {
      Name  = "${module.eks.cluster_name}-0"
      Owner = "sa_demo_scylladb_com"
    }
  }

  tag_specifications {
    resource_type = "volume"
    tags = {
      Name  = "${module.eks.cluster_name}-0"
      Owner = "sa_demo_scylladb_com"
    }
  }
}

# Launch template for non-dedicated / application node group
resource "aws_launch_template" "group_lt_1" {
  name    = "${var.prefix}group-eks-launch-template-1"
  key_name = aws_key_pair.key_pair.key_name

  block_device_mappings {
    device_name = "/dev/xvda"
    ebs {
      volume_size = var.ebsSize
      volume_type = "gp3"
    }
  }

  metadata_options {
    http_tokens               = "required"
    http_put_response_hop_limit = 2
    http_endpoint             = "enabled"
  }

  network_interfaces {
    security_groups       = [local.eks_node_security_group_id]
    delete_on_termination = true
  }

  tag_specifications {
    resource_type = "instance"
    tags = {
      Name  = "${module.eks.cluster_name}-1"
      Owner = "sa_demo_scylladb_com"
    }
  }

  tag_specifications {
    resource_type = "volume"
    tags = {
      Name  = "${module.eks.cluster_name}-1"
      Owner = "sa_demo_scylladb_com"
    }
  }
}

# Dedicated node group
resource "aws_eks_node_group" "dedicated" {
  cluster_name    = module.eks.cluster_name
  node_group_name = "dedicated"
  node_role_arn   = aws_iam_role.eks_nodes.arn

  subnet_ids      = var.singlezone ? [aws_subnet.public_subnet_1.id] : [aws_subnet.public_subnet_1.id , aws_subnet.public_subnet_2.id, aws_subnet.public_subnet_3.id ]

  instance_types  = [local.instance_type0]
  capacity_type   = var.capacity_type
  version         = var.eks_nodegroup_version
  release_version = var.arch == "arm64" ? data.aws_ssm_parameter.eks_ami_arm64.value : data.aws_ssm_parameter.eks_ami_x86.value
  ami_type        = var.arch == "arm64" ? "AL2023_ARM_64_STANDARD" : "AL2023_x86_64_STANDARD"

  launch_template {
    id      = aws_launch_template.group_lt_0.id
    version = "$Latest"
  }

  labels = {
    "scylla.scylladb.com/node-type" = "scylla"
    Owner                           = "sa_demo_scylladb_com"
    Name                            = "sa-demo-cluster"
  }

  taint {
    key    = "scylla-operator.scylladb.com/dedicated"
    value  = "scyllaclusters"
    effect = "NO_SCHEDULE"
  }

  scaling_config {
    desired_size = var.ng_0_size
    min_size     = 0
    max_size     = 3
  }
}

# Non-dedicated node group
resource "aws_eks_node_group" "non-dedicated" {
  cluster_name    = module.eks.cluster_name
  node_group_name = "non-dedicated"
  node_role_arn   = aws_iam_role.eks_nodes.arn

  subnet_ids = var.singlezone ? [aws_subnet.public_subnet_1.id] : [aws_subnet.public_subnet_1.id , aws_subnet.public_subnet_2.id, aws_subnet.public_subnet_3.id ]

  instance_types  = [local.instance_type1]
  capacity_type   = var.capacity_type
  version         = var.eks_nodegroup_version
  release_version = var.arch == "arm64" ? data.aws_ssm_parameter.eks_ami_arm64.value : data.aws_ssm_parameter.eks_ami_x86.value
  ami_type        = var.arch == "arm64" ? "AL2023_ARM_64_STANDARD" : "AL2023_x86_64_STANDARD"

  launch_template {
    id      = aws_launch_template.group_lt_1.id
    version = "$Latest"
  }

  labels = {
    "scylla.scylladb.com/node-type" = "scylla-operator"
    Owner                           = "sa_demo_scylladb_com"
    Name                            = "sa-demo-cluster"
  }

  scaling_config {
    desired_size = var.ng_1_size
    min_size     = 0
    max_size     = 3
  }
}

# Application node group (optional)
resource "aws_eks_node_group" "application" {
  count           = var.enable_application ? 1 : 0
  cluster_name    = module.eks.cluster_name
  node_group_name = "application"
  node_role_arn   = aws_iam_role.eks_nodes.arn

  subnet_ids = var.singlezone ? [aws_subnet.public_subnet_1.id] : [aws_subnet.public_subnet_1.id , aws_subnet.public_subnet_2.id, aws_subnet.public_subnet_3.id ]

  instance_types  = [local.instance_type2]
  capacity_type   = var.capacity_type
  version         = var.eks_nodegroup_version
  release_version = var.arch == "arm64" ? data.aws_ssm_parameter.eks_ami_arm64.value : data.aws_ssm_parameter.eks_ami_x86.value
  ami_type        = var.arch == "arm64" ? "AL2023_ARM_64_STANDARD" : "AL2023_x86_64_STANDARD"

  launch_template {
    id      = aws_launch_template.group_lt_1.id
    version = "$Latest"
  }

  labels = {
    "scylla.scylladb.com/node-type" = "application"
    Owner                           = "sa_demo_scylladb_com"
    Name                            = "sa-demo-cluster"
  }

  taint {
    key    = "scylla-operator.scylladb.com/dedicated"
    value  = "application"
    effect = "NO_SCHEDULE"
  }

  scaling_config {
    desired_size = var.ng_2_size
    min_size     = 0
    max_size     = 3
  }
}

# Worker node IAM role
data "aws_iam_policy_document" "eks_nodes_assume_role_policy" {
  statement {
    actions = ["sts:AssumeRole"]

    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com"]
    }
  }
}

resource "aws_iam_role" "eks_nodes" {
  name               = "${module.eks.cluster_name}-EKS-node-group-Role"
  assume_role_policy = data.aws_iam_policy_document.eks_nodes_assume_role_policy.json
}

resource "aws_iam_role_policy_attachment" "eks_worker_node" {
  for_each = toset([
    "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
    "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
    "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
    "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy",
    "arn:aws:iam::aws:policy/AmazonEC2FullAccess",
    "arn:aws:iam::aws:policy/AmazonS3FullAccess"
  ])

  role       = aws_iam_role.eks_nodes.name
  policy_arn = each.value
}

# IRSA trust policies
data "aws_iam_policy_document" "irsa_ebs_csi_trust" {
  statement {
    actions = ["sts:AssumeRoleWithWebIdentity"]

    principals {
      type        = "Federated"
      identifiers = [module.eks.oidc_provider_arn]
    }

    condition {
      test     = "StringEquals"
      variable = "${module.eks.oidc_provider}:sub"
      values   = ["system:serviceaccount:kube-system:ebs-csi-controller-sa"]
    }
  }
}

data "aws_iam_policy_document" "irsa_kube_proxy_trust" {
  statement {
    actions = ["sts:AssumeRoleWithWebIdentity"]

    principals {
      type        = "Federated"
      identifiers = [module.eks.oidc_provider_arn]
    }

    condition {
      test     = "StringEquals"
      variable = "${module.eks.oidc_provider}:sub"
      values   = ["system:serviceaccount:kube-system:kube-proxy"]
    }
  }
}

data "aws_iam_policy_document" "irsa_coredns_trust" {
  statement {
    actions = ["sts:AssumeRoleWithWebIdentity"]

    principals {
      type        = "Federated"
      identifiers = [module.eks.oidc_provider_arn]
    }

    condition {
      test     = "StringEquals"
      variable = "${module.eks.oidc_provider}:sub"
      values   = ["system:serviceaccount:kube-system:coredns"]
    }
  }
}

# IRSA roles
resource "aws_iam_role" "irsa_ebs_csi" {
  name               = "${module.eks.cluster_name}-AmazonEKSTFEBSCSIRole"
  assume_role_policy = data.aws_iam_policy_document.irsa_ebs_csi_trust.json
}

resource "aws_iam_role_policy_attachment" "irsa_ebs_csi_policy" {
  role       = aws_iam_role.irsa_ebs_csi.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
}

resource "aws_iam_role" "irsa_kube_proxy" {
  name               = "${module.eks.cluster_name}-kube-proxy"
  assume_role_policy = data.aws_iam_policy_document.irsa_kube_proxy_trust.json
}

resource "aws_iam_role" "irsa_coredns" {
  name               = "${module.eks.cluster_name}-AmazonEKSTFCorednsRole"
  assume_role_policy = data.aws_iam_policy_document.irsa_coredns_trust.json
}

# EKS addons that need IRSA we manage ourselves
resource "aws_eks_addon" "coredns" {
  depends_on               = [aws_eks_node_group.dedicated, aws_eks_node_group.non-dedicated]
  cluster_name             = module.eks.cluster_name
  addon_name               = "coredns"
  service_account_role_arn = aws_iam_role.irsa_coredns.arn
}

resource "aws_eks_addon" "ebs_csi_driver" {
  depends_on               = [aws_eks_node_group.dedicated, aws_eks_node_group.non-dedicated]
  cluster_name             = module.eks.cluster_name
  addon_name               = "aws-ebs-csi-driver"
  service_account_role_arn = aws_iam_role.irsa_ebs_csi.arn
}

data "aws_caller_identity" "current" {}

# Pod Identity role for VPC CNI
resource "aws_iam_role" "vpc_cni_pod_identity" {
  name = "${module.eks.cluster_name}-AmazonEKSPodIdentityAmazonVPCCNIRole"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Principal = {
          Service = "pods.eks.amazonaws.com"
        }
        Action = [
          "sts:AssumeRole",
          "sts:TagSession"
        ]
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "vpc_cni_policies" {
  for_each = toset([
    "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
    "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
  ])

  role       = aws_iam_role.vpc_cni_pod_identity.name
  policy_arn = each.value
}

resource "aws_eks_pod_identity_association" "vpc_cni" {
  cluster_name    = module.eks.cluster_name
  namespace       = "kube-system"
  service_account = "aws-node"
  role_arn        = aws_iam_role.vpc_cni_pod_identity.arn
}

# SG rule to metadata endpoint (egress; optional)
resource "aws_security_group_rule" "metadata_access" {
  type              = "egress"
  from_port         = 80
  to_port           = 80
  protocol          = "tcp"
  cidr_blocks       = ["169.254.169.254/32"] # EC2 metadata endpoint
  security_group_id = local.eks_node_security_group_id
}
