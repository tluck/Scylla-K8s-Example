terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
    }
    random = {
      source  = "hashicorp/random"
    }
    tls = {
      source  = "hashicorp/tls"
    }
    cloudinit = {
      source  = "hashicorp/cloudinit"
    }
  }
}

provider "aws" {
  region = var.region
}

# Fetch existing VPC and subnets
data "aws_vpc" "existing_vpc" {
  id = var.vpc_id
}

data "aws_subnets" "existing_subnets" {
  filter {
    name   = "vpc-id"
    values = [data.aws_vpc.existing_vpc.id]
  }
}

# Key pair for node access
resource "aws_key_pair" "key_pair" {
  key_name   = "${local.cluster_name}-keypair"
  public_key = file(var.ssh_public_key_file)
}

# Get EKS-optimized AL2023 AMI
data "aws_ssm_parameter" "eks_ami" {
  name = "/aws/service/eks/optimized-ami/${var.eks_version}/amazon-linux-2023/x86_64/standard/recommended/image_id"
}

locals {
  eks_cluster_security_group_id = module.eks.cluster_security_group_id
  eks_node_security_group_id    = module.eks.node_security_group_id
}

# Create MIME-formatted user data for nodeadm configuration
# data "cloudinit_config" "node_config" {
#   gzip          = true
#   base64_encode = true

#   part {
#     content_type = "application/node.eks.aws"
#     content = templatefile("${path.module}/nodeconfig.yaml.tpl", {
#       cluster_name    = local.cluster_name
#       cluster_endpoint = module.eks.cluster_endpoint
#       cluster_ca      = module.eks.cluster_certificate_authority_data
#       cluster_service_cidr    = module.eks.cluster_service_cidr
#     })
#   }
# }

# EKS cluster module
module "eks" {
  source  = "terraform-aws-modules/eks/aws"

  cluster_name                            = local.cluster_name
  cluster_version                         = var.eks_version
  cluster_endpoint_public_access          = true
  enable_cluster_creator_admin_permissions = true
  
  create_kms_key             = false
  cluster_encryption_config  = {}
  create_cloudwatch_log_group = false
  cluster_enabled_log_types   = []
  enable_irsa                = false

  vpc_id     = data.aws_vpc.existing_vpc.id
  subnet_ids = data.aws_subnets.existing_subnets.ids

  cluster_addons = {
    aws-ebs-csi-driver = { most_recent_version = true }
    kube-proxy          = { most_recent_version = true }
    coredns             = { most_recent_version = true }
  }

  node_security_group_additional_rules = {
    ingress_allow_ssh = {
      type        = "ingress"
      description = "SSH"
      from_port   = 22
      to_port     = 22
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }
    ingress_allow_https = {
      type        = "ingress"
      description = "HTTPS"
      from_port   = 443
      to_port     = 443
      protocol    = "tcp"
      cidr_blocks = ["172.31.0.0/16"]
    }
    ingress_allow_webhook = {
      type        = "ingress"
      description = "Webhook"
      from_port   = 5000
      to_port     = 5000
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }
    ingress_allow_prometheus = {
      type        = "ingress"
      description = "Prometheus"
      from_port   = 9090
      to_port     = 9090
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }
    ingress_allow_grafana = {
      type        = "ingress"
      description = "Grafana"
      from_port   = 3000
      to_port     = 3000
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }
  }

  # EKS Managed Node Groups (using native EKS managed nodes)
  eks_managed_node_groups = {
    eks_node_group_0 = {
      name                         = "${module.eks.cluster_name}-0"
      ami_type                     = "AL2023_x86_64_STANDARD"
      instance_types               = [local.instance_type0]
      capacity_type                = "SPOT"
      
      # Use all subnets for multi-AZ distribution
      subnet_ids                   = data.aws_subnets.existing_subnets.ids
      
      desired_size                 = var.ng_0_size
      min_size                     = 3
      max_size                     = 6
      
      # Enable custom launch template with user data
      create_launch_template       = true
      use_custom_launch_template   = true
      
      # Key configuration for nodeadm with custom AMI
      ami_id                      = data.aws_ssm_parameter.eks_ami.value
      enable_bootstrap_user_data  = false
      # user_data_template_path     = "${path.module}/nodeconfig.yaml.tpl"
      # user_data = base64encode(<<-EOF
        # #!/bin/bash
            
        # # Update kubelet config for static CPU policy
        # KUBELET_CONFIG="/etc/kubernetes/kubelet/config.json"
        # jq '.cpuManagerPolicy = "static"' ${KUBELET_CONFIG} > ${KUBELET_CONFIG}.tmp
        # mv -f ${KUBELET_CONFIG}.tmp ${KUBELET_CONFIG}
        # rm -f /var/lib/kubelet/cpu_manager_state 

        # # Restart kubelet to apply changes
        # systemctl restart kubelet
      # EOF
      # )
      # user_data = base64encode(<<-EOF
      #   MIME-Version: 1.0
      #   Content-Type: multipart/mixed; boundary="//"
        
      #   --//
      #   Content-Type: application/node.eks.aws
        
      #   apiVersion: node.eks.aws/v1alpha1
      #   kind: NodeConfig
      #   spec:
      #     cluster:
      #       name: ${module.eks.cluster_name}
      #       apiServerEndpoint: ${replace(module.eks.cluster_endpoint, "https://", "")}
      #       certificateAuthority: "${module.eks.cluster_certificate_authority_data}"
      #       cidr: "${module.eks.cluster_service_ipv4_cidr}"
      #     kubelet:
      #       config:
      #         cpuManagerPolicy: static
      #   --//--
      # EOF
      # )
    
      # Launch template configuration
      key_name                    = aws_key_pair.key_pair.key_name
      
      block_device_mappings = {
        xvda = {
          device_name = "/dev/xvda"
          ebs = {
            volume_size           = var.ebsSize
            volume_type          = "gp3"
            delete_on_termination = true
          }
        }
      }
      
      metadata_options = {
        http_endpoint               = "enabled"
        http_tokens                 = "required"
        http_put_response_hop_limit = 2
      }

      iam_role_additional_policies = {
        AmazonEBSCSIDriverPolicy = "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
        AmazonEC2FullAccess      = "arn:aws:iam::aws:policy/AmazonEC2FullAccess"
        tjl-nodegroup-scylla-pool-PolicyEBS = "arn:aws:iam::403205517060:policy/tjl-nodegroup-scylla-pool-PolicyEBS"
      }
      
      tags = {
        Name = "${module.eks.cluster_name}-0"
      }
    }
  }
}

# IAM permissions for managed node group
resource "aws_iam_role_policy" "node_pass_role" {
  name = "eks-node-pass-role"
  role = module.eks.eks_managed_node_groups["eks_node_group_0"].iam_role_name

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect   = "Allow"
      Action   = "iam:PassRole"
      Resource = module.eks.eks_managed_node_groups["eks_node_group_0"].iam_role_arn
    }]
  })
}

# Security group rule for metadata access
resource "aws_security_group_rule" "metadata_access" {
  type              = "egress"
  from_port         = 80
  to_port           = 80
  protocol          = "tcp"
  cidr_blocks       = ["169.254.169.254/32"]
  security_group_id = local.eks_node_security_group_id
}

output "eks_cluster_certificate_authority_data" {
  value = module.eks.cluster_certificate_authority_data
}

## Key Changes Made:

# 1. **Removed custom launch template**: Let the EKS module handle launch template creation
# 2. **Added AMI ID specification**: Required when using custom user data with managed node groups
# 3. **Enabled bootstrap user data**: Essential for custom nodeadm configuration
# 4. **Set user_data_template_path**: Points to the nodeadm template file
# 5. **Multi-AZ distribution**: Uses all subnets instead of single subnet
# 6. **Proper block device mapping**: Configured as nested object instead of separate resource
# 7. **Metadata options**: Configured within the node group instead of launch template
# 8. **Cleaned up redundant code**: Removed unused resources and commented section
